# 大语言模型训练方法

## 1. LLM微调方法

### 1.1 预训练(Pre-training)

- 定义:从头开始训练LLM,使用数万亿个数据标记
- 方法:自监督算法,通常使用自回归预测下一个标记
- 计算需求:需要数千GPU小时(10^5 - 10^7)
- 输出:基础模型(foundation model)

### 1.2 持续预训练(Continued pre-training)

- 别名:第二阶段预训练
- 方法:使用新的领域数据进一步训练基础模型
- 特点:
    - 使用与初始预训练相同的自监督算法
    - 通常涉及所有模型权重
    - 新数据与原始数据的一部分混合

### 1.3 微调(Fine-tuning)

- 定义:使用标注数据集以监督方式或强化学习技术调整预训练语言模型
- 与预训练的主要区别:
    1. 使用标注数据集进行监督训练
    2. 需要较少的标记(数千或数百万,而非数十亿或数万亿)
- 主要目标:增强指令跟随、人类对齐、任务性能等能力
- 微调的两个维度:
    1. 改变参数的百分比
    2. 添加的新能力

## 1.3.1 根据改变参数的百分比分类

1. 完全微调(Full fine-tuning)：改变模型的所有参数，适用于小型模型(如XLMR、BERT)和大型模型(如Llama 2、GPT3)
2. 参数高效微调(PEFT)：只微调少量额外参数或更新预训练参数的一个子集，通常涉及1-6%的总参数

## 1.3.2 添加到基础模型的能力

- 目的:增加预训练模型的能力,如指令跟随、人类对齐等
- 示例:经过聊天微调的Llama 2模型

### 1.4 检索增强生成(RAG)

- 定义:"搜索驱动的LLM文本生成"
- 方法:使用用户问题检索相关内容,将其注入LLM提示中
- 优势:可以添加特定领域的知识库
- 示例应用:Chat LangChain(基于LangChain文档的问答聊天机器人)

### 1.5 上下文学习(ICL)

- 方法:在提示中放置原型示例
- 有效性:多项研究表明"通过示例演示"是有效的
- 示例类型:
    - 仅输入和输出文本(少样本学习)
    - 推理轨迹(如思维链(CoT)提示)
    - 计划和反思轨迹(如ReACT)

## 2. 选择适当的适应方法

- 考虑因素:所需模型能力、训练成本、推理成本、数据集类型等
- 推荐流程:文章提供了一个决策流程图

## 3. 各方法的适用性分析

### 3.1 不推荐的方法

- 预训练(❌):原因:计算需求巨大,不适合资源有限的团队
- 持续预训练(❌):原因:仍然需要大量数据和计算资源，存在灾难性遗忘问题

### 3.2 推荐的方法

- 完全微调和参数高效微调(PEFT)(✅):
    - 优势:成本效益高,可以达到最先进的结果
    - 适用:特定任务和专业领域
- 检索增强生成(RAG)(✅):
    - 适用:需要从动态知识库提取信息的应用
    - 注意:推理成本可能较高
- 上下文学习(ICL)(✅):
    - 优势:最具成本效益,不需要额外训练数据或计算资源
    - 注意:推理成本和延迟可能增加

## 4. 总结

- 建议:从简单方法开始,逐步增加复杂性
- 过程:迭代式创建LLM系统
- 策略:文章提供的流程图可作为LLM适应策略的基础



# 你是否需要对模型进行微调

### 历史背景

- 小型模型(100M-300M参数)常用监督式微调(SFT)构建领域应用
- 大型模型(>1B参数)的微调问题更加复杂

### 微调的挑战

1. 资源需求:大型模型需要更多资源和商用硬件，示例: Llama 2 7B和13B模型的GPU内存使用情况(见表1)
2. 灾难性遗忘:全参数微调的常见问题，PEFT技术旨在解决这一问题

## 适合微调的场景

1. **定制语气、风格和格式**
    - 目的: 使LLM匹配特定角色或服务特定受众
    - 应用: 定制聊天机器人响应,输出特定格式(如JSON,YAML,Markdown)
2. **提高准确性和处理边缘情况**
    - 目的: 纠正幻觉或错误,增强新技能或任务能力
    - 示例:
        - Phi-2在金融数据情感分析上的准确率从34%提升到85%
        - ChatGPT在Reddit评论情感分析上的准确率提升25个百分点(48%到73%)
3. **处理代表性不足的领域**
    - 目的: 提高在特定领域(如法律、医疗、金融)的准确性
    - 示例:
        - 总结病人医疗历史
        - 改善印度语言等代表性不足语言的任务表现
4. **降低成本**
    - 方法:
        - 将大模型(如Llama 2 70B/GPT-4)的技能提炼到小模型(如Llama 2 7B)
        - 减少对长篇或特定提示的需求
    - 示例: 通过提炼更昂贵的GPT-4模型来微调GPT-3.5评判器
5. **新任务/能力**
    - 示例:
        - 改善LLM使用给定检索器上下文的能力
        - 微调LLM评判其他LLM的能力(如真实性、合规性、有用性)
        - 增加上下文窗口

## 与其他领域适应技术的比较

### 微调 vs. 上下文学习(少样本学习)

- 上下文学习(ICL)优势:
    - 简单,应在微调前尝试
    - 可帮助评估微调是否能改善下游任务性能
- ICL考虑因素:
    - 示例数量增加会提高推理成本和延迟
    - LLM可能忽略部分示例
    - 可能泄露示例中的知识(微调也存在此问题)

### 微调 和 检索增强生成(RAG)

- 不是简单的替代关系,而是互补的方法
- 选择考虑因素:
    1. 应用是否需要外部知识?
    2. 是否需要自定义语气/行为/词汇或风格?
    3. 对幻觉的容忍度如何?
    4. 有多少标记的训练数据?
    5. 数据的静态/动态程度如何?
    6. LLM应用需要多大的透明度/可解释性?
    7. 成本和复杂性: 团队是否有相关专业知识?
    8. 应用中任务的多样性如何?
- 建议:
    - 混合解决方案(微调+RAG)通常效果最佳
    - 考虑成本、时间和额外独立收益
    - 分析错误以了解可能的指标增益
    - 微调前需要制定数据收集和改进策略

# LLM 微调数据集准备

## 微调方法比较

### 全参数微调 vs. 参数高效微调 (PEFT)

- 两种方法都能改善下游任务性能
- 选择考虑因素:
    - 可用计算资源 (GPU小时和内存)
    - 目标任务外的其他任务表现 (学习-遗忘权衡)
    - 人工标注成本

### 全参数微调的潜在问题

1. 模型崩溃: 输出收敛到有限的集合
2. 灾难性遗忘: 模型丧失原有能力

### PEFT优势

- 自然的正则化作用
- 计算成本较低
- 更适合资源受限和数据集有限的场景

### 选择建议

- 资源受限时: PEFT可能提供更好的性能提升/成本比
- 下游任务性能至关重要: 全参数微调可能更有效
- 关键: 在任何情况下都要创建高质量数据集

## 数据集准备

### 数据质量/数量

- 质量优先于数量
- 高质量数据特征:
    - 一致的标注
    - 无错误、误标、噪声
    - 代表性分布
- 复杂语言任务需要更多数据
- 高效数据收集策略:
    - 关注失败模式
    - 人机协作标注

### 数据多样性

- 目标: 训练数据应反映模型在真实世界中的期望行为
- 关键点:
    1. 去重: 避免重复数据导致的性能下降
    2. 输入多样性: 对输入进行改写以增加语法和语义多样性
    3. 数据集多样性: 使用多样化数据集改善学习-遗忘权衡
    4. 标准化输出: 移除无关格式，专注于重要概念

### 基于LLM的数据管道

- 评估: 使用高质量数据集训练的模型筛选大规模数据集
- 生成: 使用高质量示例引导LLM生成类似的高质量样本
- 人机协作: LLM生成初始输出，人类改进质量

### 数据集调试

- 评估不良输出
- 检查正负类平衡
- 确保完整性和一致性:
    - 训练样本包含所有必要信息
    - 保持格式一致
    - 检查多人标注的一致性

## 结论

- 微调是LLM开发的关键方面，需要平衡艺术和科学
- 数据集质量和准备对微调成功至关重要
- 小型微调LLM在特定任务上可能优于大型模型
- 推荐参考Llama微调指南作为起点
- 专有数据集混合方法阻碍了最佳实践的共享
- 预期未来将出现通用最佳实践，同时保持微调的创造性和适应性

## 关键观点

1. 数据质量比数量更重要，少量高质量数据优于大量低质量数据
2. 数据多样性对防止模型偏见至关重要
3. 利用LLM辅助数据准备可以提高效率和质量
4. 持续评估和调试数据集是微调过程中的重要步骤
5. 微调仍然是一个快速发展的领域，需要不断适应和创新