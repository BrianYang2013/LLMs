



[Ollama + Open webui](https://www.mn.cyou/archives/ollama): 大语言LLM模型的本地部署方案

[open-webui](https://github.com/open-webui)/[open-webui](https://github.com/open-webui/open-webui): open-webui serve, This will start the Open WebUI server, which you can access at [http://localhost:8080](http://localhost:8080/)